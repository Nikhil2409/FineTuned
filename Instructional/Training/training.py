import os, re, time, torch, tiktoken, json
from functools import partial
from torch.utils.data import DataLoader
from Instructional.model import model, CHOOSE_MODEL, BASE_CONFIG
from Instructional.Data.data_set import InstructionDataset
from Instructional.Data.collate import custom_collate_fn
from Instructional.Training.functions import train_model_simple
from Instructional.Data.format import format_input
from Instructional.Accuracy.post_training import post_training_generate

# ðŸ”¹ Device & seed
torch.manual_seed(123)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ðŸ”¹ Tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# ðŸ”¹ Load fresh JSONs generated by data_setup.py
with open("train_data.json", "r") as f:
    train_data_json = json.load(f)
with open("val_data.json", "r") as f:
    val_data_json = json.load(f)
with open("test_data.json", "r") as f:
    test_data_json = json.load(f)

# ðŸ”¹ Wrap datasets
train_dataset = InstructionDataset(train_data_json, tokenizer)
val_dataset = InstructionDataset(val_data_json, tokenizer)
test_dataset = InstructionDataset(test_data_json, tokenizer)

# ðŸ”¹ DataLoaders
customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)
batch_size = 2

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                          drop_last=True, num_workers=0, collate_fn=customized_collate_fn)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                        drop_last=False, num_workers=0, collate_fn=customized_collate_fn)

# ðŸ”¹ Check dataset sizes
print(f"Train size: {len(train_dataset)}")
print(f"Val size: {len(val_dataset)}")
print(f"Test size: {len(test_dataset)}")

# ðŸ”¹ Checkpoint directory
base_dir = "/content/drive/MyDrive/Finetuned_checkpoints"
os.makedirs(base_dir, exist_ok=True)
checkpoint_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-sft.pth"
checkpoint_path = os.path.join(base_dir, checkpoint_name)

# ðŸ”¹ Load checkpoint if available (resume training)
#try:
#    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
#   print(f"Loaded checkpoint: {checkpoint_path}")
#except FileNotFoundError:
#   print("No checkpoint found, training from scratch.")

# ðŸ”¹ Move model to device
model = model.to(device)

# ðŸ”¹ Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.5)

# ðŸ”¹ Train
start_time = time.time()
num_epochs = 5

train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context=format_input(val_data_json[0]), tokenizer=tokenizer,
    checkpoint_path=checkpoint_path,
    grad_accum_steps=8
)
end_time = time.time()
print(f"Training completed in {(end_time - start_time)/60:.2f} minutes.")

# ðŸ”¹ Load best checkpoint for post-training generation
try:
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    print(f"Loaded best checkpoint for post-training generation.")
except FileNotFoundError:
    print("Best checkpoint not found. Using the last trained model.")

# ðŸ”¹ Generate responses
output_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-responses.json"
output_path = os.path.join(base_dir, output_name)
test_data_json = post_training_generate(model, tokenizer, device, test_data_json)
print(f"Responses saved at {output_path}")
